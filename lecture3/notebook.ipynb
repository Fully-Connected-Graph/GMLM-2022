{"cells":[{"cell_type":"markdown","metadata":{"id":"1vBfGD6addcL"},"source":["# Lecture 3 Practical\n","\n","Welcome to the practice session of Lecture 3. During this tutorial, we will use build an image classifier that works on the MNIST Fashion Dataset. The images present in this dataset are 28 x 28 pixels with gray colors. There are 10 categories which you can see in the below table\n","\n","| Category | CategoryId |\n","| --- | :---: |\n","| T-shirt | 0 |\n","| Trouser | 1 |\n","| Pullover | 2 |\n","| Dress | 3 |\n","| Coat | 4 |\n","| Sandal | 5 |\n","| Shirt | 6 |\n","| Sneaker | 7 |\n","| Bag | 8 |\n","| Ankle Boot | 9 |\n","\n","First, let's install the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXmXhpvIcw4A"},"outputs":[],"source":["# no need if you are doing in Colab\n","# !pip install tensorflow keras pandas numpy matplotlib"]},{"cell_type":"markdown","metadata":{"id":"RvE0shvq3EqD"},"source":["and import the modules that we will use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUoNiMqddtdf"},"outputs":[],"source":["# to import and manage data\n","import pandas as pd\n","import numpy as np\n","\n","# to make pretty plots\n","import matplotlib.pyplot as plt\n","\n","# to make neural networks\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"JAg9F8_X3KeK"},"source":["In this block, we set the random seed to a specific value to ensure that we get the same results every time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49yuuwn1qGXG"},"outputs":[],"source":["random_seed = 3141\n","\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"]},{"cell_type":"markdown","metadata":{"id":"UU6iLiR2hQF5"},"source":["## Loading the Data\n","\n","Since we have our environment set up, we can start working on the problem. \n","\n","The MNIST fashion is a famous starter dataset, that's why it can be accessed straight through tensorflow. Add they are already presplit into train and test sets! Checkout the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to learn more about these datasets.\n","\n","Let's acquire the features and targets for both the training and test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcpYuzWzeEsj"},"outputs":[],"source":["# Remember these variables, they will be useful later on\n","(x_train, y_train), (x_val, y_val) = keras.datasets.fashion_mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOwIYaqCjad8"},"outputs":[],"source":["id_to_label = {\n","  0: \"T-shirt/top\",\n","  1: \"Trouser\",\n","  2: \"Pullover\",\n","  3: \"Dress\",\n","  4: \"Coat\",\n","  5: \"Sandal\",\n","  6: \"Shirt\",\n","  7: \"Sneaker\",\n","  8: \"Bag\",\n","  9: \"Ankle boot\"\n","}"]},{"cell_type":"markdown","metadata":{"id":"_JvZBLdx3uML"},"source":["If we look at the dimensions of the training set, we see that there are 60000 different images. All of these comprise a 28 x 28 2d array "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoI54beZmIHe"},"outputs":[],"source":["print(x_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"iQc9t7a9liXI"},"source":["Let's inspect the image!\n","\n","<details>\n","<summary>\n","Open after you run the next cell\n","</summary>\n","\n","AHAH, very readable)))\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rsr21zTAliqp"},"outputs":[],"source":["x_train[0]"]},{"cell_type":"markdown","metadata":{"id":"aKwCDTlV3-yT"},"source":["Okay, what we can wee is that the values of the iamge are rangin from 0 to 255. To get a better idea of what we are working with, we can display this using [`plt.imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) from matplotlib."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdSYAQKrl6i7"},"outputs":[],"source":["plt.imshow(x_train[0], cmap=\"gray\"); # set cmap (color map) to gray scale"]},{"cell_type":"markdown","metadata":{"id":"a_6-zcOgl6-e"},"source":["or we can make it even better by plotting first 25 with their names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eT3TL13qpPYS"},"outputs":[],"source":["fig, axs = plt.subplots(5, 5, figsize=(10, 12))\n","\n","for i in range(5):\n","  for j in range(5):\n","    idx = i + 5 * j\n","    image = x_train[idx]\n","    label_id = y_train[idx]\n","    label = id_to_label[label_id]\n","\n","    axs[i][j].imshow(image, cmap=\"gray\") # set cmap (color map) to gray scale\n","    axs[i][j].set_title(label)           # set the title \n","    axs[i][j].axis(\"off\")                # turn off the axis"]},{"cell_type":"markdown","metadata":{"id":"c8PO_8T04f3g"},"source":["## Data Pre-processing\n","\n","Let's define some useful functions for converting the data into a workable form. There are two problems that need to be addressed.\n","- We need to normalize the scale of the pixels from 0-255 to 0-1\n","- We need to one hot encode the target values"]},{"cell_type":"markdown","metadata":{"id":"B8_VGA5D5qNz"},"source":["The pre-process function will be used to normalize the data. It is supposed to take the features and normalize them to the range 0 to 1.\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","The range of the data is from 0 to 255. There are no negative values, so we can just divide everything by the maximum value.\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","Divide the image by 255\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MS-C7FCe3DP"},"outputs":[],"source":["def preprocess_image(image, target):\n","  \"\"\"\n","  Task 1. scale the image pixel values from range 0-255 to 0-1\n","  \"\"\"\n","  normalized_image = # write code here\n","\n","  reshaped_image = normalized_image.reshape((28*28, -1))\n","\n","  return reshaped_image, target"]},{"cell_type":"markdown","metadata":{"id":"m7sJmbpu5_oL"},"source":["The `create_dataset` function will perform 3 different tasks\n","- One hot encode the categorical values\n","- Call preprocess on all images in the training set\n","- Group the data into batches for faster training\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Trivia: What are batches?\n","</summary>\n","\n","Initially, NN training needed to be done using one item at a time. However, the creation of the GPU allows several processes to be executed parallel to each other. Since the training of a neural network involves a lot of matrix multiplications, GPUs can solve this task excellently. \n","\n","A batch is a collection of data points that will be evaluated simultaneously This results in the training of a neural network to complete much faster than it would otherwise. The only limitation of using large batch sizes is that the GPU might not have enough memory.\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"1QqQ5Fq68B8o"},"source":["Most of `create_dataset` has already been implemented. The only step left is to call preprocess on all images in the dataset. This can actually be implemented in a single line!\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","Try to find out how to use [.map()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","```python\n","data.map(preprocess_image)\n","```\n","\n","</details>\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOPiMpEof0-N"},"outputs":[],"source":["def create_dataset(xs, ys, n_classes=10):\n","  ys = tf.one_hot(ys, depth=n_classes)\n","\n","  data = tf.data.Dataset.from_tensor_slices((xs, ys))\n","\n","  \"\"\"\n","  Task 2. Run the preprocess_image function over all the images in the xs\n","          This can be done using tensorflow method map on data\n","  \"\"\"\n","  \n","  processed_data = # write code here\n","  \n","  chunked_data = processed_data.batch(128)\n","\n","  return chunked_data"]},{"cell_type":"markdown","metadata":{"id":"Y0vTa0qR8W4U"},"source":["Now, that we have created our functions to process the data, we can call them and create our datasets\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","Use the `create_dataset` function defined above. What inputs does it take?\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","```python\n","train_dataset = create_dataset(x_train, y_train)\n","val_dataset = create_dataset(x_val, y_val)\n","```\n","\n","</details>\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pK1NTvsUg57T"},"outputs":[],"source":["\"\"\"\n","Task 3. Create the train and validation sets\n","\"\"\"\n","train_dataset = \n","val_dataset = "]},{"cell_type":"markdown","metadata":{"id":"O97kix_ahvGF"},"source":["## Model Creation\n","\n","Our data is ready for training, let's create our neural network. We can make a 3 layered NN to begin with and see what results we get.\n","\n","<p align=\"center\">\n","<img src=\"https://fully-connected-graph.github.io/GMLM-2022/lecture3/assets/neural_network.png\" width=\"500\"><br>\n","Example of a <a href=\"https://www.3blue1brown.com/topics/neural-networks\">neural network from 3b1b</a> for MNIST digit dataset with 2 hidden layers and 1 output layer\n","</p>\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Trivia: What is the softmax activation function?\n","</summary>\n","\n","The softmax function is an extension of sigmoid. While sigmoid can only handle binary classifcation, softmax can be used for multiple classes.\n","\n","<p align=\"center\">\n","<img src=\"https://www.nomidl.com/wp-content/uploads/2022/04/image-14.png\" alt=\"drawing\" width=\"600\"/>\n","</p>\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"QBQXJH_Y_AgQ"},"source":["Add three hidden layers in between the input and output layer with 64 neurons each. A good activation function that we can use would be ReLu.\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","You can look at how we created layers in the titanic example. The activation function for ReLu is simply \"relu\"\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","keras.layers.Dense(units=64, activation='relu')\n","\n","</details>\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkjLO5Jjhy9L"},"outputs":[],"source":["\"\"\"\n","  Task 4. Add 4 hidden layers with 64 neurons each.\n","          You can use the Dense function from the keras library\n","\"\"\"\n","\n","model = keras.Sequential([\n","    keras.layers.InputLayer(input_shape=(28 * 28)),    # input layer\n","    # write your code here\n","    keras.layers.Dense(units=10, activation='softmax') # output layer\n","])"]},{"cell_type":"markdown","metadata":{"id":"DLWZ8zax_0wr"},"source":["We can now compile our model. We will use the categorical crossentropy loss function since our target values are categorical.\n","\n","We also need to decide on a metric to judge our model\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","For categorical targets, we can check whether the predicted value is the same as the actual value\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","Use accuracy\n","\n","</details>\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPlfOXVtjhgL"},"outputs":[],"source":["\"\"\"\n","  Task 5. Decide on a metric to train the model with.\n","\"\"\"\n","model.compile(optimizer='adam', \n","              loss=tf.losses.CategoricalCrossentropy(),\n","              metrics=['Enter metric here']) # Pass the metric as an argument"]},{"cell_type":"markdown","metadata":{"id":"ufXYcRcFAZm7"},"source":["Our model is ready for training! We can fit our model on the training data and let it run for some time.\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Trivia: What are epochs?\n","</summary>\n","\n","Neural networks can gain better performance when we rerun the model on the same dataset several times. Each training session is known as an epoch. We need to make sure to not use a high number of epochs as this might lead to overfitting.\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urLDKz9ej9IO"},"outputs":[],"source":["history = model.fit(\n","    train_dataset.repeat(), \n","    epochs=10, \n","    steps_per_epoch=500,\n",")"]},{"cell_type":"markdown","metadata":{"id":"yCICMf8-BMkD"},"source":["Our model appears to have a high accuracy on the training set. But how will it fare on unseen data. Let's find out!\n","\n","Evaluate the model on unseen data. This can be done in one line\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Hint </summary>\n","\n","The evaluate method of the model can be used to evaluate test date. Check the documentation at [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n","\n","<details>\n","<summary style=\"font-size:1.5rem\"> Answer </summary>\n","\n","print(model.evaluate(val_dataset))\n","\n","</details>\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZM_qAh4kp7E"},"outputs":[],"source":["\"\"\"\n","  Task 6. Evaluate the model on the validation set\n","\"\"\"\n","# Enter code here"]},{"cell_type":"markdown","metadata":{"id":"BV8ku8eQBnyc"},"source":["To determine if our model would be useful, we can compare it to a dummy model. However, since we are dealing with categorical data, we can actually look at the most frequent element and compare it to the total population. This would give us the accuracy of the dummy model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAq9SoZYwHrF"},"outputs":[],"source":["print(np.bincount(y_train))"]},{"cell_type":"markdown","metadata":{"id":"fLzoR7w4xG9k"},"source":["Each clothing item appears equally. Thus, the accuracy of a dummy model would be 10% (6000 / 60000). This means that our model actually does something!"]},{"cell_type":"markdown","metadata":{"id":"IMbn0FTGprED"},"source":["## Manual Check\n","\n","Perhaps you want to check an image manually and see if the model predicts it. In this case, you can run the below code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAmCCNc5tJxW"},"outputs":[],"source":["predictions = model.predict(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dmu8j2Mvuk0f"},"outputs":[],"source":["print(np.argmax(predictions[random_seed]))\n","plt.imshow(x_val[random_seed], cmap=plt.cm.binary)"]},{"cell_type":"markdown","metadata":{"id":"22JgmahFCIRv"},"source":["## Conclusion\n","\n","Congratulations on reaching the end of this practical and making your own image classifier. If you wish to attain state of the art performance on image classification problems, then we suggest that you look at more modern techniques such as [Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}
